{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Function\n",
    "\n",
    "This notebook describes how I made a function to scrape information from the Greyhound Board of Great Britain Website so that I can update my dataframe.\n",
    "\n",
    "This is a two stage process.\n",
    "\n",
    "First I must see if there were any races on the dates at the desired track (Crayford as this is the main focus of my attention currently).  If so then it will get the unique race IDs for these races.\n",
    "\n",
    "Due to how the website operates, I use Selenium, the headless web browser, to enter the necessary date and track information.\n",
    "\n",
    "Once I have the race ID numbers, then I can loop through these to scrape the necessary information that I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://localhost:5432/danielpayne')\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One - Acquire Race IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This defines a function that gets the maximum number of pages from the initial page\n",
    "\n",
    "def page_number(soupObj):\n",
    "    try:\n",
    "        page = soup.find('div', class_=\"rgWrap rgInfoPart\").get_text()\n",
    "        max_page = int(page.split('in ')[1][:2])\n",
    "    except:\n",
    "        max_page = 1\n",
    "    return max_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now I can go through the dates that I need and get the Race Ids for the races.\n",
    "\n",
    "def greyhound_scraper(start_date, end_date):\n",
    "\n",
    "    # The URL to go to.\n",
    "    url = (\"http://www.gbgb.org.uk/Results.aspx\")\n",
    "\n",
    "    # Empty list for which ti append the Race IDS\n",
    "    race_ids = []\n",
    "\n",
    "\n",
    "    # open the driver for Selenium and input the url\n",
    "    driver = webdriver.Chrome(executable_path=\"../../../chromedriver\")\n",
    "    driver.get(url)\n",
    "\n",
    "    #Enter the date needed here and make sure format is correct\n",
    "    dates =  pd.date_range(start_date, end_date).map(lambda x: x.strftime('%d/%m/%Y'))\n",
    "    #dates = [start_date, end_date]\n",
    "    for date in dates:\n",
    "        # This finds the drop down menu element\n",
    "        menu = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_mainContent_cmscontent_TrackRaces_ddlTrack\"]/span/span[2]')\n",
    "        menu.click()\n",
    "        # This selects the race track.  In this case Crayford which is 6th on the drop down list\n",
    "        track = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_mainContent_cmscontent_TrackRaces_ddlTrack_DropDown\"]/div/ul/li[6]')\n",
    "        sleep(1)\n",
    "        track.click()\n",
    "        # This is the element where you can enter the date\n",
    "        date_entry = driver.find_element_by_xpath('//*[@id=\"ctl00_ctl00_mainContent_cmscontent_TrackRaces_dtpDate_dateInput\"]')\n",
    "        date_entry.clear()\n",
    "        date_entry.send_keys(date, Keys.RETURN)\n",
    "        sleep(1)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        if soup.find_all('tr', class_=(re.compile('rgRow|rgAltRow'))) == []:\n",
    "            print 'No records for:', date\n",
    "            pass\n",
    "        else:\n",
    "            n = 1\n",
    "            for i in range(page_number(soup)):                                       # Using page number function for each dog\n",
    "                if i < page_number(soup): \n",
    "                    print 'Page', n, 'of', page_number(soup), 'for', date\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                results = soup.find_all('tr', class_=(re.compile('rgRow|rgAltRow'))) # Regex to get the alternate rows info is in\n",
    "                for result in results:\n",
    "                    for race in result.find_all('td'):\n",
    "                        if race.a is not None and 'View Race' in race.a:            # Calling the info in the \"a\" tag for \"Race\"\n",
    "                            string =  str(race.a)                                    # Make into a string\n",
    "                            id_num = re.search('\\d+', string)                        # Search for any numeric value\n",
    "                            race_number = int(id_num.group())                        # Return integer of that number\n",
    "                            race_ids.append(race_number)                    # Append onto list\n",
    "                if n == page_number(soup):\n",
    "                    pass\n",
    "                else:\n",
    "                    n += 1\n",
    "                    content = driver.find_element_by_class_name('rgPageNext')            # Turning page onto next\n",
    "\n",
    "                    content.click()\n",
    "                    sleep(2)                                                        # Allow page to load\n",
    "\n",
    "    driver.close()\n",
    "    return race_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run the function as so.  This example would look the races for 14th July\n",
    "race_list = greyhound_scraper('2017-07-14', '2017-07-14')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two - Loop through race Ids and get the necessary information for my dataframe\n",
    "\n",
    "** First I define functions that can extract each required element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining functions to extract each piece of information from the racecard.\n",
    "\n",
    "def get_track(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        track = header.find(\"div\", class_=\"track\").get_text(strip=True)\n",
    "    return track\n",
    "\n",
    "def get_date(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        date = header.find(\"div\", class_=\"date\").get_text(strip=True)\n",
    "    return date\n",
    "\n",
    "def get_datetime(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        datetime = header.find(\"div\", class_=\"datetime\").get_text(strip=True)\n",
    "    return datetime\n",
    "\n",
    "def get_grade(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        grade = header.find(\"div\", class_=\"grade\").get_text(strip=True)\n",
    "    return grade\n",
    "\n",
    "def get_distance(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        distance = header.find(\"div\", class_=\"distance\").get_text(strip=True)\n",
    "    return distance\n",
    "\n",
    "def get_prizes(soupObj):\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        prizes = header.find(\"div\", class_=\"prizes\").get_text(strip=True)\n",
    "    return prizes\n",
    "\n",
    "def get_going_allowance(soupObj):\n",
    "    header = soup.find(\"div\", class_=\"resultsBlockFooter\")\n",
    "    result = header.find(\"div\").get_text(strip=True)\n",
    "    try:\n",
    "        going = result.split(':')[1]\n",
    "    except:\n",
    "        going = result\n",
    "    return going\n",
    "\n",
    "def get_dog_name(soupObj):\n",
    "    greyhound_participants = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"greyhound\").get_text(strip=True)\n",
    "            greyhound_participants.append(dog_name)\n",
    "    return greyhound_participants\n",
    "\n",
    "def get_position(soupObj):\n",
    "    position = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"first essential fin\").get_text(strip=True)\n",
    "            position.append(dog_name)\n",
    "    return position\n",
    "\n",
    "def get_trap_number(soupObj):\n",
    "    traps = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"trap\").get_text(strip=True)\n",
    "            traps.append(dog_name)\n",
    "    return traps\n",
    "    \n",
    "def get_odds(soupObj):\n",
    "    odds = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"sp\").get_text(strip=True)\n",
    "            odds.append(dog_name)\n",
    "    return odds\n",
    "    \n",
    "def get_time_trap(soupObj):\n",
    "    time_trap = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"timeSec\").get_text(strip=True)\n",
    "            time_trap.append(dog_name)\n",
    "    return time_trap\n",
    "\n",
    "def get_time_distance(soupObj):\n",
    "    time_dist = []\n",
    "    for header in soupObj.find_all(\"div\", class_=\"resultsBlockHeader\"):\n",
    "        results = header.find_next_sibling(\"div\", class_=\"resultsBlock\").find_all(\"ul\", class_=\"line1\")\n",
    "        for result in results:\n",
    "            dog_name = result.find(\"li\", class_=\"timeDistance\").get_text(strip=True)\n",
    "            time_dist.append(dog_name)\n",
    "    return time_dist\n",
    "\n",
    "def get_weight(soupObj):\n",
    "    weights = []\n",
    "    results = soup.find_all(\"ul\", class_=\"line2\")\n",
    "    for result in results:\n",
    "        comment = result.find(\"li\", class_=\"first essential\").get_text(strip=True)\n",
    "        comment = [entry for entry in comment.split('  ') if entry != '']\n",
    "        try:\n",
    "            weight_num = re.search('\\d\\d.\\d', comment[4])\n",
    "            weight = float(weight_num.group())\n",
    "        except:\n",
    "            weight = 'NA'\n",
    "        weights.append(weight)\n",
    "    return weights\n",
    "\n",
    "# This function not only retrieves the name of the Dam (mother) but also when the greyhound was born\n",
    "\n",
    "def get_dam(soupObj):\n",
    "    dams = []\n",
    "    born = []\n",
    "    results = soup.find_all(\"ul\", class_=\"line2\")\n",
    "    for result in results:\n",
    "        comment = result.find(\"li\", class_=\"first essential\").get_text(strip=True)\n",
    "        comment = [entry for entry in comment.split('  ') if entry != '']\n",
    "        if len(comment) > 3:\n",
    "            dam_data = comment[3]\n",
    "            dam_data = dam_data.split(' ')\n",
    "            try:\n",
    "                born_data = dam_data[-2]\n",
    "            except:\n",
    "                born_data = 'NA'\n",
    "            dam_data = ' '.join(dam_data[:-2])\n",
    "            born.append(born_data)\n",
    "            dams.append(dam_data)\n",
    "        else:\n",
    "            born.append('nan')\n",
    "            dams.append('nan')\n",
    "    return dams, born\n",
    "\n",
    "# This function not only gets the Sire (father) of the dog but also a random element possibly the sex\n",
    "\n",
    "def get_sire(soupObj):\n",
    "    sires = []\n",
    "    random = []\n",
    "    results = soup.find_all(\"ul\", class_=\"line2\")\n",
    "    for result in results:\n",
    "        comment = result.find(\"li\", class_=\"first essential\").get_text(strip=True)\n",
    "        comment = [entry for entry in comment.split('  ') if entry != '']\n",
    "        if len(comment) > 1:\n",
    "            sire_data = comment[1]\n",
    "            sires.append(sire_data[2:])\n",
    "            random.append(sire_data[:2])\n",
    "        else:\n",
    "            sires.append('nan')\n",
    "            random.append('nan')\n",
    "    return sires, random\n",
    "\n",
    "def get_trainer(soupObject):\n",
    "    trainers = []\n",
    "    results = soup.find_all(\"ul\", class_=\"line2\")\n",
    "    for result in results:\n",
    "        tr_info = result.find(\"li\", class_=\"essential trainer\").get_text(strip=True)\n",
    "        trainer = tr_info.split(':')[1]\n",
    "        trainers.append(trainer[:-1])\n",
    "    return trainers   \n",
    "\n",
    "def get_comment(soupObj):\n",
    "    comments = []\n",
    "    results = soup.find_all(\"ul\", class_=\"line3\")\n",
    "    for result in results:\n",
    "        comment_info = result.find(\"li\", class_=\"first essential comment\").get_text(strip=True)\n",
    "        comment_info = comment_info.split(':')[1]\n",
    "        comments.append(comment_info)\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a copy of the list so don't have to change code below!!\n",
    "gbgb_race_ids = race_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Url stem to which I can add the Race Ids to loop through racecards. \n",
    "url_stem = 'http://www.gbgb.org.uk/resultsRace.aspx?id='\n",
    "\n",
    "# Define a dictionary that will hold all racecard information\n",
    "gbgb_dict = {'Race_id' : [],\n",
    "            'Name': [],\n",
    "            'Position' : [],\n",
    "            'Sire': [],\n",
    "            'Dam' : [],\n",
    "            'Born' : [],\n",
    "            'Random' : [],\n",
    "            'Trap_no' : [],\n",
    "            'Odds' : [],\n",
    "            'Time_trap' : [],\n",
    "            'Time_distance' : [],\n",
    "            'Weight' : [],\n",
    "            'Track' : [],\n",
    "            'Trainer' : [],\n",
    "            'Comment' : [],\n",
    "            'Date' : [],\n",
    "            'Datetime' : [],\n",
    "            'Grade' : [],\n",
    "            'Distance' : [],\n",
    "            'Prizes' : [],\n",
    "            'Going_allowance': []}\n",
    "\n",
    "i = 1\n",
    "for race in gbgb_race_ids:\n",
    "    Url = url_stem + str(race)\n",
    "    page = requests.get(Url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    gbgb_names = get_dog_name(soup)                                                   # Using functions defined earlier\n",
    "    if gbgb_names is not None:                                                        # Check in case of empty result\n",
    "        if i % 50 == 0:\n",
    "            print i, 'out of', len(gbgb_race_ids), 'race_id :', gbgb_race_ids[i]  # Progress counter\n",
    "        gbgb_dict['Name'].append(gbgb_names)                            \n",
    "        gbgb_dict['Race_id'].append([race] * len(gbgb_names))                         # Append Race_id to each dog entry\n",
    "        gbgb_dict['Position'].append(get_position(soup))\n",
    "        gbgb_dict['Sire'].append(get_sire(soup)[0])\n",
    "        gbgb_dict['Dam'].append(get_dam(soup)[0])\n",
    "        gbgb_dict['Born'].append(get_dam(soup)[1])\n",
    "        gbgb_dict['Random'].append(get_sire(soup)[1])\n",
    "        gbgb_dict['Trap_no'].append(get_trap_number(soup))\n",
    "        gbgb_dict['Odds'].append(get_odds(soup))\n",
    "        gbgb_dict['Time_trap'].append(get_time_trap(soup))\n",
    "        gbgb_dict['Time_distance'].append(get_time_distance(soup))\n",
    "        gbgb_dict['Weight'].append(get_weight(soup))\n",
    "        gbgb_dict['Track'].append([get_track(soup)] * len(gbgb_names))              # Some values appear only once\n",
    "        gbgb_dict['Trainer'].append(get_trainer(soup))\n",
    "        gbgb_dict['Comment'].append(get_comment(soup))\n",
    "        gbgb_dict['Date'].append([get_date(soup)] * len(gbgb_names))\n",
    "        gbgb_dict['Datetime'].append([get_datetime(soup)] * len(gbgb_names))\n",
    "        gbgb_dict['Grade'].append([get_grade(soup)] * len(gbgb_names))\n",
    "        gbgb_dict['Distance'].append([get_distance(soup)] * len(gbgb_names))\n",
    "        gbgb_dict['Prizes'].append([get_prizes(soup)] * len(gbgb_names))\n",
    "        gbgb_dict['Going_allowance'].append([get_going_allowance(soup)] * len(gbgb_names))\n",
    "        i += 1\n",
    "    else:\n",
    "        print 'Race_id to pass:', gbgb_race_ids[i]\n",
    "        i += 1\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a copy as the dictionary is temporary\n",
    "test = gbgb_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each element is a list of lists so I need to expand these out into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each element is a list of lists so they are appended together to make one list.  Lengths are checked for equality.\n",
    "\n",
    "for key in test.keys():\n",
    "    test[key] = [a for b in test[key] for a in b]\n",
    "    \n",
    "for key in test.keys():\n",
    "    print key, len(test[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make into a dataframe\n",
    "race_update = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save to an SQL database\n",
    "race_update.to_csv('crayford_race_update_raw.csv', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
